Backlog:
1. Implement a check for linear separability that can be used by the perception network
2. Research for better ways to optimize the Hebb rule than using the pseudoinverse rule - for example the delta rule
3. Implement a positive learning rate (alpha) and a decay term (gamma) which will limit the weight matrix and prevent it from holding big values
4. Implement a check for the linear associator which will be able to determine which is the closest pattern to the input. Hamming distance?
5. The Hebb rule does not work when the input vectors are not orthogonal and the input vectors contain values different than -1 and 1 (tests 07 - 10).
6. Write own method for eigenvalues calculation instead of using the Jama library.
7. LMS: implement algorithm which will take into consideration the probabilities of input vector occurrences.

----------------------------------------------
Library:
Neurocomputing: Foundations of Research by John Anderson and Edward Rosenfeld

Notes:
The brain consists of approximately Math.pow(10, 11) neurons. Each neuron has approximately Math.pow(10, 4) connections.
Each neuron has 3 principal components:
 - dendrites: tree-like receptive networks of nerve fibers that carry electrical signals into the cell body
 - cell body: effectively sums and thresholds these incoming signals
 - axon: single long fiber that carries the signal from the cell body out to other neurons. The point of contact between an axon of one cell and a dendrite of another cell is called a synapse

Biological neurons compared to electrical circuit: Math.pow(10, -3) s compared to Math.pow(10, -10) s.

A single (composite) layer of neurons can be defined with different transfer functions by combining two different networks in parallel. Both networks would have the same inputs, and each network would create some of the outputs.

A layer whose output is the network output is called an output layer. The other layers are called hidden layers.

The bias gives the network an extra variable, and as a result networks with biases would be more powerful than those without. For instance, a neuron without
a bias will always have a net input of zero when the network inputs are zero. This may not be desirable and can be avoided by the use of a bias.

How to Pick an Architecture?
Problem specifications help define the network in the following ways:
1. Number of network inputs = number of problem inputs
2. Number of neurons in output layer = number of problem outputs
3. Output layer transfer function choice at least partly determined by problem specification of the outputs

Perception network:
The decision boundary is the line consisting of all points for which the net input n is equal to 0.
The decision boundary will always be orthogonal to the weight matrix, and the position of the boundary can be shifted by changing the bias b.
The key characteristic of the single-layer perceptron is that it creates linear decision boundaries to separate categories of input vector. The multilayer networks are able to solve classification problems of arbitrary complexity.
A single-neuron perceptron can classify input vectors into two categories. With multiple-neuron perception there are a total of Math.pow(2, S) possible categories, where S is the number of neurons.

Hamming Network:
It was designed explicitly to solve binary pattern recognition problems (where each element of the input vector has only two possible values â€” in our example 1 or ). It uses both feedforward and recurrent (feedback) layers, as the number of neurons in both layers is the same.
There is one neuron in the recurrent layer for each prototype pattern. When the recurrent layer converges, there will be only one neuron with nonzero output.
 - Feedforward Layer:
   The feedforward layer performs a correlation, or inner product, between each of the prototype patterns and the input pattern.
   In order for the feedforward layer to perform this correlation, the rows of the weight matrix in the feedforward layer, represented by the connection matrix, are set to the prototype patterns.
   The feedforward layer uses a linear transfer function, and each element of the bias vector is equal to the number of elements in the input vector.
   By adding the number of elements in the input vector to the inner product we guarantee that the outputs of the feedforward layer can never be negative. This is required for proper operation of the recurrent layer.
   This network is called the Hamming network because the neuron in the feedforward layer with the largest output will correspond to the prototype pattern that is closest in Hamming distance to the input pattern. (The Hamming distance between two vectors is equal to the number of elements that are different. It is defined only for binary vectors.) We leave it to the reader to show that the outputs of the feedforward layer are equal to twice the number of elements in the input vector minus twice the Hamming distances from the prototype patterns to the input pattern.
 - Recurrent Layer:
   The recurrent layer of the Hamming network is what is known as a â€œcompetitiveâ€� layer - the neurons compete with each other to determine a winner. After the competition, only one neuron will have a nonzero output.
   Since the outputs of successive iterations produce the same result, the network has converged.

Hopfield Network:
This is a recurrent network that is similar in some respects to the recurrent layer of the Hamming network, but which can effectively perform the operations of both layers of the Hamming network.
The neurons in this network are initialized with the input vector, then the network iterates until the output converges.
Whereas in the Hamming network the nonzero neuron indicates which prototype pattern is chosen, the Hopfield network actually produces the selected prototype pattern at its output.
